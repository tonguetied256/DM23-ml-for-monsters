{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fafe4c74-8730-4352-84f4-810cdc1e6368",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Coding a Neural Net from Scratch: Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cf71a9-bd36-4454-899a-7933d82a2ff2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Code Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dc5da4a-91e1-4cac-a6e2-ccd1d26fa3ce",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88f44800-70bb-4856-9615-598ad89391bd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7de7ae9c-9ef1-417c-a74a-fc5ad209ded0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6025029c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources\n",
    "\n",
    "Work forked from Jake Tae with modifications, additions and clarifications: \n",
    "\n",
    "- [Jake Tae](https://jaketae.github.io): [Word2vec from Scratch ](https://jaketae.github.io/study/word2vec)\n",
    "\n",
    "Other resources: \n",
    "- [Eric Kim](https://aegis4048.github.io): [Demystifying Neural Network in Skip-Gram Language Modeling](https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9184fd5c-ffb5-4285-afb6-e4c6e3cde1f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "- Today we'll implement the Word2Vec model from scratch in Numpy\n",
    "- Word2vec was introduced in a 2013 paper and sent shock waves through the NLP world, along with the similar GloVe model\n",
    "- Given a word, Word2Vec predicts it context (or vice versa). \n",
    "- However, this isn't the main point of the model:\n",
    "- Goal: Produce a good representation of language through the LM task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fe06ac-4c3d-4749-909e-2742abfe5bb4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Architectures\n",
    "\n",
    "- Word2vec has two separate architectures:\n",
    "    - **Skipgram (SG)**: Predict context from target\n",
    "    - **Disitributed Bag-of-Words (DBOW)**: Predict target from context\n",
    "- There has since then been new iterations of W2V\n",
    "    - **Paraghaph2Vec / Doc2Vec:** Vectors for entire documents\n",
    "    - **FastText:** Skipgram but using ngram level tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2825258-2d3d-4b54-a2bb-073fc0ad0ae0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Today We Will Learn\n",
    "    \n",
    "- We will first implement the standard version of Skipgram with a toy example\n",
    "- Then we'll expand the model to work with a larger dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0119c4f-c498-4470-90d6-3681d18a187f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why?\n",
    "\n",
    "- Word2Vec encapsulates the basic logic of the language modelling paradigm\n",
    "- Easy to implement and understand, unlike larger and more complex models like BERT or GPT\n",
    "- Implementing it in Numpy we'll build intuition, later we'll use Torch for all our neural nets\n",
    "    - You'll get an intro to Torch in the classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a6a85a-35fe-457c-8534-b4a961a17848",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Premise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc3776d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**The Task:** *Can we write a neural network that predicts a word, given its context?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7892c157",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<div>\n",
    "<img src=\"../img/skip-gram-paper.png\" width=\"30%\"\n",
    "     caption=\"test\"/>\n",
    "    <figcaption>Original Skip-gram model architecture.</figcaption>\n",
    "</div>\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c609ba-1bcf-4029-8c74-23603bbb1dd9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The Goal:** Produce vector representations that capture semantic (and other) relationships between words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cfb631-331a-4ad2-a947-11fa0e7f1af5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> The man who passes the sentence should swing the sword. - Ned Stark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99ee393-4092-45c1-af00-5479bf62ec77",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<div>\n",
    "<img src=\"../img/word2vec3d.png\" width=\"100%\"\n",
    "     caption=\"test\"/>\n",
    "    <figcaption>Original Producing data for the skipgram model.</figcaption>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2244aa-debc-4773-b30c-634979355995",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intuition: Steps with 1 Training Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e70cd1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e71823be-71f8-4560-bfee-82a777b8a8ee",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "text = '''The man who passes the sentence should swing the sword.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a2feb4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As so often, let's start by defining a tokenizer and tokenizing our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcdb00a9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2425b79a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tokens = tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a96b34f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e759cc04",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'man', 'who', 'passes', 'the', 'sentence', 'should', 'swing', 'the', 'sword']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc3636fc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7ba2c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, much like encoding things in bits, we can't just give a model the tokens as such. \n",
    "- Tokens need to be somehow intelligible for mathematical analysis.\n",
    "- A common way of doing this, is to just give each unique token a number. \n",
    "    - This creates a two-way map, where each token has a number and each number has a token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773b90d2-9a7b-4752-8c95-48cb00ecc1cb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**PROMPT:** Write a function that returns two dictionaries:\n",
    "    \n",
    "1. ```word2idx```: Maps each unique token to a numerical index\n",
    "2. ```idx2word```: Maps each idx to its corresponding token\n",
    "\n",
    "**Hint:** Use ```enumerate```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03d724f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef mapping(tokens):\\n    tokens = set(tokens)\\n    word2idx = { i : token for (i,token) in enumerate(tokens) } \\n    idx2word = { token : i for (i,token) in enumerate(tokens) }\\n    return word2idx, idx2word\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mapping(tokens):\n",
    "    word2idx, idx2word = {}, {}\n",
    "\n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word2idx[token] = i\n",
    "        idx2word[i] = token\n",
    "    \n",
    "    return word2idx, idx2word\n",
    "\n",
    "\"\"\"\n",
    "def mapping(tokens):\n",
    "    tokens = set(tokens)\n",
    "    word2idx = { i : token for (i,token) in enumerate(tokens) } \n",
    "    idx2word = { token : i for (i,token) in enumerate(tokens) }\n",
    "    return word2idx, idx2word\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffecba21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "word2idx, idx2word = mapping(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33de26ce",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sentence', 0),\n",
       " ('man', 1),\n",
       " ('sword', 2),\n",
       " ('swing', 3),\n",
       " ('should', 4),\n",
       " ('who', 5),\n",
       " ('the', 6),\n",
       " ('passes', 7)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word2idx.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8179eb35",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx['sword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00041c19",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'sentence'),\n",
       " (1, 'man'),\n",
       " (2, 'sword'),\n",
       " (3, 'swing'),\n",
       " (4, 'should'),\n",
       " (5, 'who'),\n",
       " (6, 'the'),\n",
       " (7, 'passes')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(idx2word.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "856e8c50",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sentence'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04234e1b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In a sense, we have created a lookup table that allows us to easily convert from words to indices, and indices to words. This will come in handy later!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef70351-ebd7-45e7-a21c-74aa547dd693",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training Data\n",
    "\n",
    "- Now we want to create the input data for our neural network\n",
    "- What w2v needs, is a sliding window with a target word and context words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3cbdb0-81fd-466c-b3c8-009e55c5fc54",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<div>\n",
    "<img src=\"../img/quote_ned_stark.png\" width=\"100%\"\n",
    "     caption=\"test\"/>\n",
    "    <figcaption>Original Producing data for the skipgram model.</figcaption>\n",
    "</div>\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e188be-ba80-4137-aa03-17bc2160f987",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It's rare that we have to generate this data and that's not the main exercise today, so we'll use a helper function for that.\n",
    "- We will generate ```X``` and ```Y```\n",
    "    - **X:** Our target words\n",
    "    - **Y:** The corresponding context words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7924b65b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(id, vocab_size):\n",
    "    res = [0] * vocab_size\n",
    "    res[id] = 1\n",
    "    return res\n",
    "\n",
    "def concat(*iterables):\n",
    "    for iterable in iterables:\n",
    "        yield from iterable\n",
    "\n",
    "def generate_training_data(tokens, word_to_id, window):\n",
    "    X = []\n",
    "    y = []\n",
    "    n_tokens = len(tokens)\n",
    "    for i in range(n_tokens):\n",
    "        idx = concat(\n",
    "            range(max(0, i - window), i), \n",
    "            range(i, min(n_tokens, i + window + 1))\n",
    "        )\n",
    "        for j in idx:\n",
    "            if i == j:\n",
    "                continue\n",
    "            X.append(one_hot_encode(word_to_id[tokens[i]], len(word_to_id)))\n",
    "            y.append(one_hot_encode(word_to_id[tokens[j]], len(word_to_id)))\n",
    "    \n",
    "    return np.asarray(X), np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e460fbe1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'man',\n",
       " 'who',\n",
       " 'passes',\n",
       " 'the',\n",
       " 'sentence',\n",
       " 'should',\n",
       " 'swing',\n",
       " 'the',\n",
       " 'sword']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = generate_training_data(tokens, word2idx, 2) \n",
    "# not counting window size both directs so |window| \n",
    "# w context work is 4\n",
    "\n",
    "\n",
    "tokens\n",
    "#word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c92323-96ea-4cab-a8e5-0958eacc14b7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Okay, let's take a look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79e3934b-cf55-455a-8a18-70e5bd750618",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b8f6e04",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec22725-a43e-4c7c-83e1-fbd5677ce9f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**PROMPT:** What is going on in ```X```?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfadce55",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<!--- YOUR THOUGHTS HERE ---->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e304a9f6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b96d492e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a107ee15-00e8-4321-bcf7-49ad36fb2ba7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Weight matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9968c28e-abcc-4644-9448-02739beec034",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In addition to our training data, we need to initialize the weight matrices. \n",
    "- Our network has one hidden layer, so we need two weights matrices. \n",
    "- The are initialized with ```np.random.randn``` because we want the initial values to be random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d98310e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def init_network(vocab_size, n_embedding):\n",
    "    model = {\n",
    "        \"W_input\": np.random.randn(vocab_size, n_embedding),\n",
    "        \"W_output\": np.random.randn(n_embedding, vocab_size)\n",
    "    }\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "627e2d18",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = init_network(len(word2idx), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd43311c-c53b-4df0-938b-8e6fa19bec3f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Each row in the matrix represents a word. The three columns represent the values in the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11dcc66e-72a1-423b-af29-878968bf0d1e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.236005  ,  0.46965926, -1.15377188],\n",
       "       [-0.33913932,  0.51213087,  0.52194861],\n",
       "       [-1.25638325, -0.90652628,  0.12963666],\n",
       "       [-0.5815026 ,  0.73465797, -0.26087624],\n",
       "       [-0.23206693,  0.1710204 , -0.95887281],\n",
       "       [ 0.57932129, -0.89890758, -0.94040184],\n",
       "       [-0.7777377 ,  0.78065895, -0.31655094],\n",
       "       [-0.18456406,  0.89402227, -1.5501263 ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['W_input']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c429cf-f546-4c64-9829-c1c4ae618206",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<div>\n",
    "<img src=\"../img/embedding_matrix_input.png\" width=\"30%\"\n",
    "     caption=\"test\"/>\n",
    "    <figcaption>Original Producing data for the skipgram model.</figcaption>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2129068-51f2-4045-b841-2b6720dfc32e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d017e996-e301-4e76-995c-13aa675eb865",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's first take a look at the big picture. We'll walk over this step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d712f9-dd3f-45a5-9ca1-8d87710dd184",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<div>\n",
    "<img src=\"../img/word2vec.png\" width=\"100%\"\n",
    "     caption=\"test\"/>\n",
    "    <figcaption>Matrix operations for the Skipgram model.</figcaption>\n",
    "</div>\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856ba149",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f21e0cb-24e6-4a81-bd87-20f55c658d2f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3792abc-d394-4497-bf8c-5aa5eb2e5c70",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We begin by creating the hidden layer.\n",
    "- We use $x$ to select the correct embedding from $W_{input}$. \n",
    "    - The hidden lauer $h$ is just this embedding!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e96b3d5-99ee-4c4a-b0f5-f5764e8d5e5d",
   "metadata": {},
   "source": [
    "<center>\n",
    "<div>\n",
    "<img src=\"../img/x_w_input_h.png\" width=\"80%\"\n",
    "     caption=\"test\"/>\n",
    "    <figcaption>Producing the hidden layer.</figcaption>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2e4046-7365-4fa2-8c45-1f58b5208d47",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The matrices need to have appropriate shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "113539c3-cd36-46f4-a047-b1fed3093e55",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,)\n",
      "(8, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X[0].shape)\n",
    "print(model['W_input'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c3c97f-0681-4c90-876c-4878fc4e71b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**PROMPT:** How would you create the hidden layer using Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264234f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now the hidden layer is in turn multiplied by the matrix of the context vectors. \n",
    "\n",
    "First we check that the shapes match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91b2f25b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"W_output\"].shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defe7ed1-8878-4c74-aa50-778e7099d558",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "They do. **However:** In the figure above we needed to tranpose ```W_output```. But, because Numpy gives ```h``` as a row vector, we don't have to do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "439f2649-7e7d-415a-a473-8a2405ebe9be",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.7777377 ,  0.78065895, -0.31655094])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = X[0] @ model[\"W_input\"]\n",
    "print(h.shape)\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdaa09a-d621-4633-8652-999784f96d84",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So we can proceed to multiply our hidden layer with the next set of weights. These are the embeddings for the context words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0ab50de",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O = h @ model[\"W_output\"]\n",
    "O.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab07a30f-8480-4200-b3ea-be3de9356feb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we have our output, which isn't showed in the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef4a0020-f3b6-48c1-af09-9dedb42302d4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.69551536,  0.50237487, -1.58765667,  0.21637934, -0.78123597,\n",
       "       -1.20804658,  2.47952191,  0.33015407])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c7c428-3752-4240-869c-915c68fc2342",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**PROMPT:** What do these values indicate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f232fed7-c554-4143-911b-0ad5ae48eeb8",
   "metadata": {},
   "source": [
    "<!--- YOUR THOUGHTS HERE ---->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdb2a64",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And that's it! All that remains for us to have predictions, is to squeeze each row into a probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b1414a-e275-4a53-b03d-face8e42fab1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Activation: Softmax "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dd3e0a-ffea-452b-a487-007481a6f0fd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<div>\n",
    "<img src=\"../img/softmax_function.png\" width=\"80%\"\n",
    "     caption=\"test\"/>\n",
    "    <figcaption>We take the softmax of the output layer.</figcaption>\n",
    "</div>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6af7585-62c1-48cb-b0de-53d218ea51b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To squeeze the values in the output vector into a probability distribution, we pass it through the softmax. I've created a function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c65fa6-fda0-453d-be8b-9fa11eb31c9f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def softmax_one(x):\n",
    "    exp = np.exp(x)\n",
    "    return exp / exp.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb00a482-0fc8-4ae4-8dc0-be9fe40b1a5d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And now, *finally*, we can get our actual predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005822cd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = softmax_one(O)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16070f6b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are 8 words in the vocabulary, so we have a prediction for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385bfc1c-59dc-4b13-89eb-9866f0e71d91",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519541a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The predictions sum to 1, so it's a proper probability distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a7107a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.sum(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15111dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And the max and min values in the array are bound by 0 and 1, as they should be if they express probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0352535",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "max(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ddf857",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "min(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7247caaf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can wrap everything up and write a function for the forward pass of our neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b99620",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def forward(model, x, return_fwd=True):\n",
    "    fwd_pass = {}\n",
    "    fwd_pass[\"h\"] = x @ model[\"W_input\"]\n",
    "    fwd_pass[\"O\"] = fwd_pass[\"h\"] @ model[\"W_output\"]\n",
    "    fwd_pass[\"y_pred\"] = softmax_one(fwd_pass[\"O\"])\n",
    "    if not return_fwd:\n",
    "        return fwd_pass[\"y_pred\"]\n",
    "    return fwd_pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4670d926",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b5d03e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To pass corrections backward to the different layers of the model, we are going to use two familiar tools:\n",
    "\n",
    "1. Cross-Entropy loss\n",
    "2. The Chain Rule of Calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906186f9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can start by defining cross entropy. For this model, the two distributions that we are comparing are:\n",
    "    \n",
    "1. Our correct labels ```y```\n",
    "2. And our predicted values ```y_pred```\n",
    "\n",
    "So we can write our cross entropy function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbff200",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy(y_pred, y):\n",
    "    return - np.sum(np.log(y_pred) * y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da298ac-e82a-4cce-aa10-69fe10fa1dee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we apply the chain rule and if $L_{CE}$ stands for cross-entropy loss, then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c655bd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\frac{\\partial L_{CE}}{\\partial W_{i}} =\n",
    "\\frac{\\partial L_{CE}}{\\partial Y_{pred}} \\cdot\n",
    "\\frac{\\partial Y_{pred}}{\\partial O} \\cdot\n",
    "\\frac{\\partial O}{\\partial W_{o}} \\cdot\n",
    "\\frac{\\partial W_{o}}{\\partial H} \\cdot\n",
    "\\frac{\\partial H}{\\partial W_{i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33103b6b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Effectively what we do with the cross entropy, is that we \"wrap\" the entire neural net\n",
    "\n",
    "$$\n",
    "H = X \\cdot W_{input} \\\\\n",
    "O = H \\cdot W_{output} \\\\\n",
    "Y_{pred} = \\sigma (O)\n",
    "$$\n",
    "\n",
    "But we could also write this as:\n",
    "\n",
    "$$\n",
    "Y_{pred} = \\sigma (W_{output}(X \\cdot W_{input} ))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ca3aa4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So when we take the derivative of cross entropy, we should replace $z$ with this function. Seems tricky! And indeed, it's a pretty long and cumbersom mathematical derivation which we won't get into. But the outcome is actually very beautiful:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ba5370",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\frac{\\partial O}{\\partial W_o} = Y_{pred} - Y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c74aec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So to calculate our corrections to each matrix that produces $y_pred$, we start by subtracting ```y``` from ```y_pred```. This gives us the gradient for the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8677ec9b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dO = y_pred - Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f4e34c-1f68-4fad-afbe-5ddccf01ab2b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34bdfb9-24a0-4d30-86ab-806fbc591602",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For the other derivatives, we get:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial H}{\\partial W_{i}} = H^T \\times \\frac{\\partial O}{\\partial W_o}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial W_{i}}{\\partial X} = X^T \\times \\frac{\\partial H}{\\partial W_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ebeac7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "However, what we're really trying to find are the gradients for our embedding matrices, i.e. ```dw_target``` and ```dw_context```. To find them, we have to apply the chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e787e23",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dw_output = h[np.newaxis].T @ dO[np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbf39fc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dh = dO @ model[\"W_output\"].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae5ea24",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dw_input = X[0][np.newaxis].T @ dh[np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318e21ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, we set the learning rate, multiply it with our respective gradients and subtract the product from our embedding matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a82b8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2ba887",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model[\"W_input\"] -= lr * dw_input\n",
    "model[\"W_output\"] -= lr * dw_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e4d998",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can wrap all of this in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5d3434",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def backward(model, x, y, alpha):\n",
    "    fwd_pass  = forward(model, x)\n",
    "    dO = fwd_pass[\"y_pred\"] - y\n",
    "    dw_output = fwd_pass[\"h\"][np.newaxis].T @ dO[np.newaxis]\n",
    "    dh = dO @ model[\"W_output\"].T\n",
    "    dw_input = x[np.newaxis].T @ dh[np.newaxis]\n",
    "    assert(dw_output.shape == model[\"W_output\"].shape)\n",
    "    assert(dw_input.shape == model[\"W_input\"].shape)\n",
    "    model[\"W_input\"] -= alpha * dw_input\n",
    "    model[\"W_output\"] -= alpha * dw_output\n",
    "    return cross_entropy(fwd_pass[\"y_pred\"], y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a8e095",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fb9651",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = init_network(len(word2idx), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672a80a3-4516-4c54-81a6-a6786c8be7a1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 500\n",
    "learning_rate = 0.05\n",
    "history = []\n",
    "\n",
    "for epoch in range(0, n_epochs):\n",
    "    for idx, _ in enumerate(X):\n",
    "        history.append(backward(model, X[idx], Y[idx], learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6c8393",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(history)), history, color=\"skyblue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc3a8a8-c1c7-4a08-b60d-444b389b11b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond One Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff241ff8-8ad8-4d79-aaf3-ff69cb755db8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Normally we really don't want to train a neural net one example at a time. We did that for purely pedagogical reasons. \n",
    "\n",
    "If we want to instead run the network for a longer text and train all examples at once, we could take a longer example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319753ed-1a6a-4d6d-909b-c7a86e7dfa69",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "text = '''Machine learning is the study of computer algorithms that \\\n",
    "improve automatically through experience. It is seen as a \\\n",
    "subset of artificial intelligence. Machine learning algorithms \\\n",
    "build a mathematical model based on sample data, known as \\\n",
    "training data, in order to make predictions or decisions without \\\n",
    "being explicitly programmed to do so. Machine learning algorithms \\\n",
    "are used in a wide variety of applications, such as email filtering \\\n",
    "and computer vision, where it is difficult or infeasible to develop \\\n",
    "conventional algorithms to perform the needed tasks.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab10a02-1a1d-4275-b174-01d02b5b276c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tokens_ = tokenize(text)\n",
    "word2idx_, idx2word_ = mapping(tokens_)\n",
    "X_, Y_ = generate_training_data(tokens_, word2idx_, 2)\n",
    "model_ = init_network(len(word2idx_), 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612dbf3f-a55e-4e4d-abba-53953dbd7bc5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We redefine softmax so it accepts an entire matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5cd8e2-b9af-4957-adb9-11e699724e08",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def softmax_batch(X):\n",
    "    res = []\n",
    "    for x in X:\n",
    "        exp = np.exp(x)\n",
    "        res.append(exp / exp.sum())\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0d26bd-53ac-43b1-9bfb-254410e0f001",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And same for the forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f501b71-01e0-42e9-886c-2847e0a90bd7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def forward(model, X, return_fwd=True):\n",
    "    fwd_pass = {}\n",
    "    fwd_pass[\"h\"] = X @ model[\"W_input\"]\n",
    "    fwd_pass[\"O\"] = fwd_pass[\"h\"] @ model[\"W_output\"]\n",
    "    fwd_pass[\"y_pred\"] = softmax_batch(fwd_pass[\"O\"])\n",
    "    \n",
    "    if not return_fwd:\n",
    "        return fwd_pass[\"y_pred\"]\n",
    "    return fwd_pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d899a0f-7c88-4318-b063-0451c3e56483",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And the backward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdff7da-bea7-4746-b330-721adab1c091",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def backward(model, X, y, alpha):\n",
    "    fwd_pass  = forward(model, X)\n",
    "    dO = fwd_pass[\"y_pred\"] - y\n",
    "    dw2 = fwd_pass[\"h\"].T @ dO\n",
    "    dh = dO @ model[\"W_output\"].T\n",
    "    dw1 = X.T @ dh\n",
    "    assert(dw2.shape == model[\"W_output\"].shape)\n",
    "    assert(dw1.shape == model[\"W_input\"].shape)\n",
    "    model[\"W_input\"] -= alpha * dw1\n",
    "    model[\"W_output\"] -= alpha * dw2\n",
    "    return cross_entropy(fwd_pass[\"y_pred\"], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf7e0c2-0dea-4127-a26a-c15bcca5e242",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can initialize the model again with this data. Let's make the vectors longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f962616-ddb4-433c-80c5-40bf93d573b7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_ = init_network(len(word2idx_), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c478acf3-c653-48c4-8df6-5c5652d5fae3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's train an plot the loss over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b923511a-4042-47ad-b97e-029906fa9f33",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs_ = 500\n",
    "learning_rate_ = 0.05\n",
    "\n",
    "history = [backward(model_, X_, Y_, learning_rate_) for _ in range(n_epochs_)]\n",
    "\n",
    "plt.plot(range(len(history)), history, color=\"skyblue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff716fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "learning = one_hot_encode(word2idx_[\"learning\"], len(word2idx_))\n",
    "result = forward(model_, [learning], return_fwd=False)[0]\n",
    "\n",
    "for word in (idx2word_[id] for id in np.argsort(result)[::-1][:10]):\n",
    "    print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841118e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_embedding(model, word, word2idx):\n",
    "    try:\n",
    "        idx = word2idx[word]\n",
    "    except KeyError:\n",
    "        print(\"`word` not in corpus\")\n",
    "    one_hot = one_hot_encode(idx, len(word2idx))\n",
    "    return forward(model, one_hot)[\"h\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ebc70e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "get_embedding(model_, \"machine\", word2idx_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5c9fc6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating a model class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcdd02a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So far we have kept the code fragmented in order to understand the different pieces. But this is not how you usually write an ML model. Instead, it's customary to wrap the model in an appropriate class with most relevant functions as methods. Let's do that now for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f02669f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class Word2VecSG():\n",
    "    def __init__(self, X, Y, vocab_size, n_embedding, batch_size):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.w_target = np.random.randn(vocab_size, n_embedding)\n",
    "        self.w_context = np.random.randn(n_embedding, vocab_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.X_batches = self.create_batches(self.X)\n",
    "        self.Y_batches = self.create_batches(self.Y)\n",
    "        \n",
    "    def _softmax(self, X):\n",
    "        res = []\n",
    "        for x in X:\n",
    "            exp = np.exp(x)\n",
    "            res.append(exp / exp.sum())\n",
    "        return res\n",
    "\n",
    "    def create_batches(self, M):\n",
    "        #xrange = len(self.X)\n",
    "        batches = []\n",
    "        for i in range(0, len(M), self.batch_size):\n",
    "            batches.append(M[i:i+self.batch_size])\n",
    "        return batches\n",
    "    \n",
    "    def cross_entropy(self, y_pred, y):\n",
    "        return -np.sum(np.log(y_pred) * y)\n",
    "    \n",
    "    def forward(self, X_batch):\n",
    "        fwd_pass = {}\n",
    "\n",
    "        fwd_pass[\"h\"] = X_batch @ self.w_target\n",
    "        fwd_pass[\"O\"] = fwd_pass[\"h\"] @ self.w_context\n",
    "        fwd_pass[\"y_pred\"] = self._softmax(fwd_pass[\"O\"])\n",
    "        return fwd_pass\n",
    "    \n",
    "    def backward(self, lr):\n",
    "        for idx, X_batch in enumerate(self.X_batches):\n",
    "            fwd_pass = self.forward(X_batch)\n",
    "            da2 = fwd_pass[\"y_pred\"] - self.Y_batches[idx]\n",
    "            dw2 = fwd_pass[\"h\"].T @ da2\n",
    "            da1 = da2 @ self.w_context.T\n",
    "            dw1 = X_batch.T @ da1\n",
    "            assert(dw2.shape == self.w_context.shape)\n",
    "            assert(dw1.shape == self.w_target.shape)\n",
    "            self.w_target -= lr * dw1\n",
    "            self.w_context -= lr * dw2\n",
    "            #print(max(fwd_pass[\"z\"][0]))\n",
    "            return self.cross_entropy(fwd_pass[\"y_pred\"], self.Y_batches[idx])\n",
    "    \n",
    "    def train(self, n_iter, lr):\n",
    "        self.loss = []\n",
    "        for _ in range(n_iter):\n",
    "            loss = self.backward(lr)\n",
    "            self.loss.append(loss)\n",
    "        #self.history = [self.backward(lr) for _ in range(n_iter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "484a4563-7b2c-46b4-bc37-3fbb8e147b8f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#pubdate 2010-05-29 19:31:46 +0200\n",
      "#author Tiqqun\n",
      "#SORTauthors Tiqqun\n",
      "#title The Cybernetic Hypothesis\n",
      "#LISTtitle Cybernetic Hypothesis\n",
      "#lang en\n",
      "#SORTtopics cybernetics, technology\n",
      "#source Retrieved on May 29, 2010 from [[http://cybernet.jottit.com/][cybernet.jottit.com]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We can imagine a time when the machine of governance would replace  for better or worse, who knows?  the insufficiency of the minds and devices of politics that are customary today.\n",
      " Father Dominique Dubarle, Le Monde, December 28th, 1948\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "There is a striking contrast between the conceptual refinement and dedication characterizing scientific and technical reasoning and the summary and imprecise style that characterizes political reasoning... One even asks oneself whether this is a kind of unsurpassable situation marking the definitive limits of rationality, or if one may hope that this impotence might be overcome someday and collective life be entirely rationalized.\n",
      " An encyclopedist cybernetician writing in the 1970s.\n",
      "\n",
      "\n",
      "\n",
      "** I\n",
      "\n",
      "\n",
      "\n",
      "There is probably no domain of mans thinking or material activity that cybernetics will not come to have a role in someday.\n",
      "Georges Boulanger, Dossier on Cybernetics: utopia or science of tomorrow in the world today, 1968.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The world circumscribing us [the circumverse] aims to have stable circuits, equal cycles, the expected repetitions, and trouble-free compatibility.  It intends to eliminate all partial impulses and immobilize bodies.  Parallel to this, Borges discussed the anxiety of the emperor who wanted to have such an exact map of the empire that he would have to go back over his territory at all its points and bring it up to scale, so much so that the monarchs subjects spent as much time and energy detailing it and maintaining it that the empire itself fell into ruins to the exact extent that its cartographical overview was perfected  such is the madness of the great central Zero, its desire to immobilize bodies that can only ever be \n"
     ]
    }
   ],
   "source": [
    "from bs4 import *\n",
    "\n",
    "import requests\n",
    "\n",
    "url = 'https://theanarchistlibrary.org/library/tiqqun-the-cybernetic-hypothesis.muse'\n",
    "res = requests.get(url)\n",
    "html_page = res.text\n",
    "\n",
    "# Parse the source code using BeautifulSoup\n",
    "soup = BeautifulSoup(html_page, 'html.parser')\n",
    "\n",
    "# Extract the plain text content\n",
    "text_tiqqun = soup.get_text()\n",
    "\n",
    "# Print the plain text\n",
    "print(text_tiqqun[:2000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5699510a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With the model class written, we can initialize the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c74886-f762-4157-86a2-4751046b560b",
   "metadata": {},
   "source": [
    "**PROMPT:** Tokenize the inputs, initialize word2idx & idx2word, generate the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7693d868-7b96-45ac-aeac-bfb6107d9ef0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3015664547.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[38], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    embedding_size= # YOUR CODE HERE\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "embedding_size= 50\n",
    "tokens_tiqqun = # YOUR CODE HERE\n",
    "word2idx_tiqqun, idx2word_tiqqun = # YOUR CODE HERE\n",
    "X_tiqqun, Y_tiqqun = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b167d9df-9ab2-408e-9567-7ba00ce4ae89",
   "metadata": {},
   "source": [
    "**PROMPT:** Initialize an instance of the model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c208331",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "w2v_tiqqun = Word2VecSG(\n",
    "# YOUR CODE HERE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1133dfac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll train the model using the model method ```train``` and store the loss in the ```loss``` attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c991c38-302a-4a2e-95e2-189eb7715771",
   "metadata": {},
   "source": [
    "**PROMPT:** Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b50d3-28f3-42b3-a7e1-fbc3d7dec216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47ec2ec",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "w2v_tiqqun.loss[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bca5d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The results will be similar, but keeping track of all the internals of the model is now much easier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fbc4bf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(w2v_tiqqun.loss)), w2v_tiqqun.loss, color=\"skyblue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b06422-ea3d-4772-9762-26f99f861fa0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_embeddings(model, tokens, word2idx, embedding_size):\n",
    "    def get_embedding(model, word, word2idx):\n",
    "        try:\n",
    "            idx = word2idx[word]\n",
    "        except KeyError:\n",
    "            print(\"`word` not in corpus\")\n",
    "        one_hot = one_hot_encode(idx, len(word2idx))\n",
    "        return one_hot @ model.w_target\n",
    "    n_unique_tokens = len((list(set(tokens))))\n",
    "    embeddings = np.zeros((n_unique_tokens, embedding_size))\n",
    "    for word in list(set(tokens)):\n",
    "        embeddings[word2idx[word]] = get_embedding(model, word, word2idx)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0341152-b133-4b61-8d9e-4d507737003d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "embeddings_tiqqun = get_model_embeddings(w2v_tiqqun, tokens_tiqqun, word2idx_tiqqun, embedding_size)\n",
    "embeddings_tiqqun.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc82e4-9249-4c73-8367-cd3eaa8f05a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "knn = NearestNeighbors(n_neighbors=10, metric='cosine', algorithm='auto')\n",
    "knn.fit(embeddings)\n",
    "\n",
    "def nearest_neighbors(knn, embeddings, word):\n",
    "    neighbor_indeces = knn.kneighbors([embeddings[word2idx_tiqqun[\"cybernetic\"]]],return_distance=False)\n",
    "    indeces = []\n",
    "    for idx in neighbor_indeces[0]:\n",
    "        indeces.append(idx2word_tiqqun[idx])\n",
    "    return indeces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18f3d08-98d4-4135-98a5-d8bd168e6a85",
   "metadata": {},
   "source": [
    "**PROMPT:** What are the nearest neighbors of the word 'cybernetic'. How does this change when you train the model with more data?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
