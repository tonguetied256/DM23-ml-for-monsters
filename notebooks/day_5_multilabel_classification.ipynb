{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd84b04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import gensim.downloader as api\n",
    "import torchmetrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd464e9",
   "metadata": {},
   "source": [
    "We want to use the whole dataset to do multilabel classification.\n",
    "\n",
    "multilabel classification is distinct from multiclass. \n",
    "\n",
    "In multiclass classification, we want to choose one label from a set of possibilities.\n",
    "\n",
    "In multilabel classification, we might have multiple 'true' labels for a single example.\n",
    "\n",
    "A post can be toxic, sarcastic, and obscene."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b2016b",
   "metadata": {},
   "source": [
    "# Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e083fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_input(ex, word_embeddings):\n",
    "    ex = ex.lower()\n",
    "    tokenized_ex = list(filter(lambda x: x != '', ex.rstrip().split()))\n",
    "    vecs = []\n",
    "    for word in tokenized_ex:\n",
    "        try:\n",
    "            vec = word_embeddings[word]\n",
    "        except KeyError: # this token is not in our embeddings dictionary\n",
    "            vec = np.zeros(word_embeddings.vector_size)\n",
    "        vecs.append(vec)\n",
    "    \n",
    "    centroid = np.mean(vecs, axis=0)\n",
    "    \n",
    "    # we need torch form which is a tensor, not a numpy array\n",
    "    \n",
    "    torch_tensor = torch.from_numpy(centroid).float()\n",
    "    return torch_tensor\n",
    "\n",
    "\n",
    "def form_outputs(examples):\n",
    "    ys = examples.select_columns(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n",
    "    outputs = []\n",
    "    for y in ys:\n",
    "        labels = torch.tensor(list(y.values()), dtype=torch.float)\n",
    "        outputs.append(labels)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27e533f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset jigsaw_toxicity_pred (/Users/gabriellachronis/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-e7673256ded46692/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 214.52it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"jigsaw_toxicity_pred\", data_dir='../data/jigsaw-toxic-comment-classification-challenge/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48a74759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0., 0., 0., 0., 0., 0.]),\n",
       " tensor([0., 0., 0., 0., 0., 0.]),\n",
       " tensor([0., 0., 0., 0., 0., 0.]),\n",
       " tensor([0., 0., 0., 0., 0., 0.]),\n",
       " tensor([0., 0., 0., 0., 0., 0.]),\n",
       " tensor([0., 0., 0., 0., 0., 0.]),\n",
       " tensor([1., 1., 1., 0., 1., 0.]),\n",
       " tensor([0., 0., 0., 0., 0., 0.]),\n",
       " tensor([0., 0., 0., 0., 0., 0.]),\n",
       " tensor([0., 0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "form_outputs(dataset['train'])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a9987ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the model and return as object ready for use\n",
    "# word_embeddings = api.load(\"glove-twitter-25\")\n",
    "word_embeddings = api.load(\"glove-twitter-100\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53abe894",
   "metadata": {},
   "source": [
    "# Building the Pytorch Model\n",
    "\n",
    "Here is an example FFNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1a82e8",
   "metadata": {},
   "source": [
    "here is our old model and training loop for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9e29833",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines the core neural network for doing multiclass classification over a single datapoint at a time. This consists\n",
    "    of matrix multiplication, tanh nonlinearity, another matrix multiplication, and then\n",
    "    a log softmax layer to give the ouputs. Log softmax is numerically more stable. If you take a softmax over\n",
    "    [-100, 100], you will end up with [0, 1], which if you then take the log of (to compute log likelihood) will\n",
    "    break.\n",
    "\n",
    "    The forward() function does the important computation. The backward() method is inherited from nn.Module and\n",
    "    handles backpropagation.\n",
    "    \"\"\"\n",
    "    def __init__(self, word_embeddings, inp, hid, out):\n",
    "        \"\"\"\n",
    "        Constructs the computation graph by instantiating the various layers and initializing weights.\n",
    "\n",
    "        :param inp: size of input (integer)\n",
    "        :param hid: size of hidden layer(integer)\n",
    "        :param out: size of output (integer), which should be the number of classes\n",
    "        \"\"\"\n",
    "        super(FFNN, self).__init__()\n",
    "        self.V = nn.Linear(inp, hid)\n",
    "        # self.g = nn.Tanh()\n",
    "        self.g = nn.ReLU()\n",
    "        self.W = nn.Linear(hid, out)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Initialize weights according to a formula due to Xavier Glorot.\n",
    "        nn.init.xavier_uniform_(self.V.weight)\n",
    "        nn.init.xavier_uniform_(self.W.weight)\n",
    "\n",
    "        # Initialize with zeros instead\n",
    "        # nn.init.zeros_(self.V.weight)\n",
    "        # nn.init.zeros_(self.W.weight)\n",
    "        \n",
    "        self.num_classes = out\n",
    "        self.loss = nn.BCELoss()\n",
    "        self.word_embeddings = word_embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Runs the neural network on the given data and returns log probabilities of the various classes.\n",
    "\n",
    "        :param x: a [inp]-sized tensor of input data\n",
    "        :return: an [out]-sized tensor of log probabilities. (In general your network can be set up to return either log\n",
    "        probabilities or a tuple of (loss, log probability) if you want to pass in y to this function as well\n",
    "        \"\"\"\n",
    "        return self.sigmoid(self.W(self.g(self.V(x))))\n",
    "\n",
    "    def predict(self,example) -> int:\n",
    "        x = form_input(example, self.word_embeddings)\n",
    "        log_probs = self.forward(x)\n",
    "        return log_probs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f9b5eb",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "\n",
    "We use AUROC as our primary evaluation metric. Accuracy is not a faithful metric for data as imbalanced as ours, but we still report on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81edab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import MultilabelAUROC\n",
    "from torchmetrics.classification import MultilabelAccuracy\n",
    "\n",
    "def print_evaluation(golds, predictions):\n",
    "    \"\"\"\n",
    "    Prints evaluation statistics comparing golds and predictions, each of which is a sequence of 0/1 labels.\n",
    "    Prints accuracy as well as precision/recall/F1 of the positive class, which can sometimes be informative if either\n",
    "    the golds or predictions are highly biased.\n",
    "\n",
    "    :param golds: gold labels, list of ints\n",
    "    :param predictions: pred labels, list of ints\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #print(golds)\n",
    "    #print(predictions)\n",
    "    accuracy = MultilabelAccuracy(num_labels=6, threshold=0.5)\n",
    "    acc = accuracy(predictions, golds)\n",
    "    \n",
    "    metric = MultilabelAUROC(num_labels=6, average=\"macro\", thresholds=None)\n",
    "    auroc = metric(predictions, golds)\n",
    "    output_str = \"\"\n",
    "    output_str += \";\\nAUROC: %f;\\n\" % auroc\n",
    "    output_str += \";\\nAccuracy: %f;\\n\" % acc\n",
    "\n",
    "    #output_str += \";\\nPrecision (fraction of predicted positives that are correct): %i / %i = %f\" % (num_pos_correct, num_pred, prec)\n",
    "    #output_str += \";\\nRecall (fraction of true positives predicted correctly): %i / %i = %f\" % (num_pos_correct, num_gold, rec)\n",
    "    #output_str += \";\\nF1 (harmonic mean of precision and recall): %f;\\n\" % f1\n",
    "    print(output_str)\n",
    "    return metric, auroc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee5784d",
   "metadata": {},
   "source": [
    "# Lab: Coding the Training Loop\n",
    "\n",
    "Your task will be to write the training loop for training the feed forward neural network.\n",
    "\n",
    "\n",
    "The next cell constructs the training data and test data and sets hyperparameters. Feel free to alter hyperparameters to try improving the score!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80c0bca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'],\n",
      "    num_rows: 159571\n",
      "})\n",
      "159571 / 63978 / 63978 train/dev/test examples\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'word_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# load embeddings\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#word_embeddings = model_glove_twitter\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# get the data in the right shape\u001b[39;00m\n\u001b[1;32m     14\u001b[0m train_xs \u001b[38;5;241m=\u001b[39m train_exs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomment_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 15\u001b[0m train_xs \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mform_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_embeddings\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_xs\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     16\u001b[0m train_ys \u001b[38;5;241m=\u001b[39m form_outputs(train_exs)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_ys[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# load embeddings\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#word_embeddings = model_glove_twitter\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# get the data in the right shape\u001b[39;00m\n\u001b[1;32m     14\u001b[0m train_xs \u001b[38;5;241m=\u001b[39m train_exs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomment_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 15\u001b[0m train_xs \u001b[38;5;241m=\u001b[39m [form_input(x, \u001b[43mword_embeddings\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m train_xs]\n\u001b[1;32m     16\u001b[0m train_ys \u001b[38;5;241m=\u001b[39m form_outputs(train_exs)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_ys[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# load training data\n",
    "\n",
    "train_exs = dataset['train']\n",
    "print(train_exs)\n",
    "dev_exs = dataset['test']\n",
    "test_exs = dataset['test']\n",
    "print(repr(len(train_exs)) + \" / \" + repr(len(dev_exs)) + \" / \" + repr(len(test_exs)) + \" train/dev/test examples\")\n",
    "\n",
    "\n",
    "# get the data in the right shape\n",
    "train_xs = train_exs['comment_text']\n",
    "train_xs = [form_input(x, word_embeddings) for x in train_xs]\n",
    "train_ys = form_outputs(train_exs)\n",
    "\n",
    "print(train_ys[0])\n",
    "\n",
    "dev_xs = dev_exs['comment_text']\n",
    "dev_ys = form_outputs(dev_exs)\n",
    "\n",
    "# set hyperparameters\n",
    "\n",
    "num_epochs = 10\n",
    "hidden_size = 200\n",
    "#lr = 0.001\n",
    "#lr = 0.1\n",
    "initial_learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f1ff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## training loop for toxicity classification\n",
    "\n",
    "num_classes = 6\n",
    "\n",
    "ffnn = FFNN(word_embeddings, word_embeddings.vector_size, hidden_size, num_classes)\n",
    "optimizer = optim.Adam(ffnn.parameters(), lr=initial_learning_rate)\n",
    "\n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    ex_indices = [i for i in range(0, len(train_xs))]\n",
    "    random.shuffle(ex_indices)\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    ffnn.train()\n",
    "    for idx in ex_indices:\n",
    "        \n",
    "        x = train_xs[idx]\n",
    "        y = train_ys[idx]\n",
    "\n",
    "        \n",
    "        # Zero out the gradients from the FFNN object. *THIS IS VERY IMPORTANT TO DO BEFORE CALLING BACKWARD()*\n",
    "        ffnn.zero_grad()\n",
    "        y_hat = ffnn.forward(x)\n",
    "        \n",
    "        # Can also use built-in NLLLoss as a shortcut here but we're being explicit here\n",
    "        loss = ffnn.loss(y_hat, y)\n",
    "        total_loss += loss\n",
    "        \n",
    "        # Computes the gradient and takes the optimizer step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Total loss on epoch: %f\" % (total_loss))\n",
    "    \n",
    "    ffnn.eval()\n",
    "    dev_y_hats = [ffnn.predict(ex) for ex in dev_xs]\n",
    "    #print(dev_ys)\n",
    "    #print(dev_y_hats)\n",
    "    print_evaluation(torch.stack(dev_ys).type(torch.int), torch.stack(dev_y_hats))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442f25d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_ys[0].type()\n",
    "#dev_y_hats[0].type()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11baa63f",
   "metadata": {},
   "source": [
    "# Improving the Model with Features\n",
    "\n",
    "Does the model do better with the features that you used in the binary classification task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22fe614",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e44546d",
   "metadata": {},
   "source": [
    "# Appendix: Example Training Loop\n",
    "\n",
    "Here is an example training loop for learning the XOR function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf4034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE THE DATA\n",
    "# Synthetic data for XOR: y = x1 XOR x2\n",
    "train_xs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "train_ys = np.array([0, 1, 1, 0], dtype=np.float32)\n",
    "\n",
    "# Define some constants\n",
    "# Inputs are of size 2\n",
    "feat_vec_size = 2\n",
    "\n",
    "# Let's use 4 hidden units\n",
    "embedding_size = 4\n",
    "\n",
    "# We're using 2 classes. What's presented here is multi-class code that can scale to more classes, though\n",
    "# slightly more compact code for the binary case is possible.\n",
    "num_classes = 2\n",
    "\n",
    "\n",
    "# set hyperparameters\n",
    "num_epochs = 100\n",
    "ffnn = FFNN(feat_vec_size, embedding_size, num_classes)\n",
    "initial_learning_rate = 0.1\n",
    "optimizer = optim.Adam(ffnn.parameters(), lr=initial_learning_rate)\n",
    "\n",
    "\n",
    "# RUN TRAINING\n",
    "for epoch in range(0, num_epochs):\n",
    "    \n",
    "    ex_indices = [i for i in range(0, len(train_xs))]\n",
    "    random.shuffle(ex_indices)\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for idx in ex_indices:\n",
    "        x =  torch.from_numpy(train_xs[idx]).float()\n",
    "        y = train_ys[idx]\n",
    "        # Build one-hot representation of y. Instead of the label 0 or 1, y_onehot is either [0, 1] or [1, 0]. This\n",
    "        # way we can take the dot product directly with a probability vector to get class probabilities.\n",
    "        y_onehot = torch.zeros(num_classes)\n",
    "        \n",
    "        # scatter will write the value of 1 into the position of y_onehot given by y\n",
    "        y_onehot.scatter_(0, torch.from_numpy(np.asarray(y,dtype=np.int64)), 1)\n",
    "        # Zero out the gradients from the FFNN object. *THIS IS VERY IMPORTANT TO DO BEFORE CALLING BACKWARD()*\n",
    "        ffnn.zero_grad()\n",
    "        log_probs = ffnn.forward(x)\n",
    "        \n",
    "        # Can also use built-in NLLLoss as a shortcut here but we're being explicit here\n",
    "        loss = torch.neg(log_probs).dot(y_onehot)\n",
    "        \n",
    "        total_loss += loss\n",
    "        # Computes the gradient and takes the optimizer step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Total loss on epoch %i: %f\" % (epoch, total_loss))\n",
    "    \n",
    "\n",
    "# Evaluate on the train set\n",
    "# RUN TRAINING AND TEST\n",
    "\n",
    "train_correct = 0\n",
    "for idx in range(0, len(train_xs)):\n",
    "    x = form_input(train_xs[idx])\n",
    "    y = train_ys[idx]\n",
    "    log_probs = ffnn.forward(x)\n",
    "    prediction = torch.argmax(log_probs)\n",
    "    if y == prediction:\n",
    "        train_correct += 1\n",
    "    print(\"Example \" + repr(train_xs[idx]) + \"; gold = \" + repr(train_ys[idx]) + \"; pred = \" +\\\n",
    "          repr(prediction) + \" with probs \" + repr(log_probs))\n",
    "print(repr(train_correct) + \"/\" + repr(len(train_ys)) + \" correct after training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1cb468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
