{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bd84b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import random\n",
    "import gensim.downloader as api\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232076f8",
   "metadata": {},
   "source": [
    "# The Task: 'Toxicity' Modeling\n",
    "\n",
    "In 2016 Google published a Kaggle competition called 'toxic comments', offering a prize to the user who could make the best model for predicting whether a given comment was toxic.\n",
    "\n",
    "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n",
    "\n",
    "By offering a prize for the best model, google employed a shadow labor force of skilled workers---over 5000 submissions to date. \n",
    "\n",
    "kaggle is a good place to look for datasets. \n",
    "\n",
    "The data comes from Wikipedia talk pages. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6c06cd",
   "metadata": {},
   "source": [
    "This dataset is already stored in huggingface: \n",
    "    https://huggingface.co/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cf06e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset jigsaw_toxicity_pred (/Users/gabriellachronis/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-e7673256ded46692/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 22.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"jigsaw_toxicity_pred\", data_dir='../data/jigsaw-toxic-comment-classification-challenge/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5076c5d2",
   "metadata": {},
   "source": [
    "Some examples labeled toxic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6649e7f2",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "## Let's examine our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7cdf40",
   "metadata": {},
   "source": [
    "the data get returned to us as a Dataset object. ie an instance of the huggingface Datasets class. To see what you can do see the documentation here: https://huggingface.co/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e72b2f9",
   "metadata": {},
   "source": [
    "in the huggingface datasets library, a split is a specific subset of a dataset like train and test. List a dataset’s split names with the get_dataset_split_names() function. This data is already segmented for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dac579f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'test']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import get_dataset_split_names\n",
    "\n",
    "get_dataset_split_names(\"jigsaw_toxicity_pred\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1d6ab3",
   "metadata": {},
   "source": [
    "let's look at the first entry in the train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38a1d106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comment_text': \"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\",\n",
       " 'toxic': 0,\n",
       " 'severe_toxic': 0,\n",
       " 'obscene': 0,\n",
       " 'threat': 0,\n",
       " 'insult': 0,\n",
       " 'identity_hate': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at just train\n",
    "train = dataset['train']\n",
    "\n",
    "# Get the first row in the train set. \n",
    "train[0] #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6534e5ce",
   "metadata": {},
   "source": [
    "Use the - operator to start from the end of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1745971e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comment_text': '\"\\nAnd ... I really don\\'t think you understand.  I came here and my idea was bad right away.  What kind of community goes \"\"you have bad ideas\"\" go away, instead of helping rewrite them.   \"',\n",
       " 'toxic': 0,\n",
       " 'severe_toxic': 0,\n",
       " 'obscene': 0,\n",
       " 'threat': 0,\n",
       " 'insult': 0,\n",
       " 'identity_hate': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the last row in the train set\n",
    "\n",
    "train[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0afaa15",
   "metadata": {},
   "source": [
    "Indexing by the column name returns a list of all the values in the column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18a63df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"comment_text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04201dc",
   "metadata": {},
   "source": [
    "You can combine row and column name indexing to return a specific value at a position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "540ab419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0][\"comment_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9810e271",
   "metadata": {},
   "source": [
    "But it is important to remember that indexing order matters, especially when working with large audio and image datasets. Indexing by the column name returns all the values in the column first, then loads the value at that position. For large datasets, it may be slower to index by the column name first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cce0ea",
   "metadata": {},
   "source": [
    "### Aside: dataframes\n",
    "\n",
    "We are using the built in Datasets class from hugginface, which is useful because it allows us to download a ton of datasets right in Jupyter. But plain old Pandas dataframes are very powerful for storing, examining, manipulating, and transforming data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec16e9d6",
   "metadata": {},
   "source": [
    "Make a dataframe from our Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc12466e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "104af003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hey... what is it..\\n@ | talk .\\nWhat is it......</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bye! \\n\\nDon't look, come or think of comming ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>You are gay or antisemmitian? \\n\\nArchangel WH...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159494</th>\n",
       "      <td>\"\\n\\n our previous conversation \\n\\nyou fuckin...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159514</th>\n",
       "      <td>YOU ARE A MISCHIEVIOUS PUBIC HAIR</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159541</th>\n",
       "      <td>Your absurd edits \\n\\nYour absurd edits on gre...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159546</th>\n",
       "      <td>\"\\n\\nHey listen don't you ever!!!! Delete my e...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159554</th>\n",
       "      <td>and i'm going to keep posting the stuff u dele...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15294 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  toxic  \\\n",
       "6            COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1   \n",
       "12      Hey... what is it..\\n@ | talk .\\nWhat is it......      1   \n",
       "16      Bye! \\n\\nDon't look, come or think of comming ...      1   \n",
       "42      You are gay or antisemmitian? \\n\\nArchangel WH...      1   \n",
       "43               FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!      1   \n",
       "...                                                   ...    ...   \n",
       "159494  \"\\n\\n our previous conversation \\n\\nyou fuckin...      1   \n",
       "159514                  YOU ARE A MISCHIEVIOUS PUBIC HAIR      1   \n",
       "159541  Your absurd edits \\n\\nYour absurd edits on gre...      1   \n",
       "159546  \"\\n\\nHey listen don't you ever!!!! Delete my e...      1   \n",
       "159554  and i'm going to keep posting the stuff u dele...      1   \n",
       "\n",
       "        severe_toxic  obscene  threat  insult  identity_hate  \n",
       "6                  1        1       0       1              0  \n",
       "12                 0        0       0       0              0  \n",
       "16                 0        0       0       0              0  \n",
       "42                 0        1       0       1              1  \n",
       "43                 0        1       0       1              0  \n",
       "...              ...      ...     ...     ...            ...  \n",
       "159494             0        1       0       1              1  \n",
       "159514             0        0       0       1              0  \n",
       "159541             0        1       0       1              0  \n",
       "159546             0        0       0       1              0  \n",
       "159554             0        1       0       1              0  \n",
       "\n",
       "[15294 rows x 7 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['toxic']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1097ce82",
   "metadata": {},
   "source": [
    "## Multi-label vs Binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ceed85",
   "metadata": {},
   "source": [
    "Each example is a dictionary with comment text and then 6 annotations, which are coded for either 1 or 0. This is a multilabel classification problem. We are going to simplify it first into a binary classification task. Given a post, is it 'toxic' or not? \n",
    "\n",
    "What are the inputs to the model? what is the output?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b157a360",
   "metadata": {},
   "source": [
    "# Pre-processing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5f1cd7",
   "metadata": {},
   "source": [
    "We have to transform our text inputs to numbers. How have we been doing this so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbc0ca0",
   "metadata": {},
   "source": [
    "<img src=\"../img/fullyconnected.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7703efc",
   "metadata": {},
   "source": [
    "Another way to think of the layer of input neurons is as a vector. in fact, this is how we represent the weights in each layer of the network.\n",
    "\n",
    "How can we represent the comment text as a vector? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9468420",
   "metadata": {},
   "source": [
    "## Loading embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996102e2",
   "metadata": {},
   "source": [
    "We can use embeddings.\n",
    "\n",
    "Let's load our word embeddings that we trained on X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "270593db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMISSING\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "MISSING\n",
    "\"\"\"\n",
    "\n",
    "# load word2vec embeddings we trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b04d8c3",
   "metadata": {},
   "source": [
    "Alternately, we can use pretrained vectors or embeddings downloaded from the internet. We can use Word2Vec, or GloVe, which is a model that came out a few years later and works very well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4c8220",
   "metadata": {},
   "source": [
    "We use `gensim` which is a great library for working with embeddings and training topic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b599bf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = api.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe50cca",
   "metadata": {},
   "source": [
    "Print out available models (i.e. embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "103936b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__testing_word2vec-matrix-synopsis (-1 records): [THIS IS ONLY FOR TESTING] Word vecrors ...\n",
      "conceptnet-numberbatch-17-06-300 (1917247 records): ConceptNet Numberbatch consists of state...\n",
      "fasttext-wiki-news-subwords-300 (999999 records): 1 million word vectors trained on Wikipe...\n",
      "glove-twitter-100 (1193514 records): Pre-trained vectors based on  2B tweets,...\n",
      "glove-twitter-200 (1193514 records): Pre-trained vectors based on 2B tweets, ...\n",
      "glove-twitter-25 (1193514 records): Pre-trained vectors based on 2B tweets, ...\n",
      "glove-twitter-50 (1193514 records): Pre-trained vectors based on 2B tweets, ...\n",
      "glove-wiki-gigaword-100 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
      "glove-wiki-gigaword-200 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
      "glove-wiki-gigaword-300 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
      "glove-wiki-gigaword-50 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
      "word2vec-google-news-300 (3000000 records): Pre-trained vectors trained on a part of...\n",
      "word2vec-ruscorpora-300 (184973 records): Word2vec Continuous Skipgram vectors tra...\n"
     ]
    }
   ],
   "source": [
    "for model_name, model_data in sorted(info['models'].items()):\n",
    "    print(\n",
    "        '%s (%d records): %s' % (\n",
    "            model_name,\n",
    "            model_data.get('num_records', -1),\n",
    "            model_data['description'][:40] + '...',\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc82140d",
   "metadata": {},
   "source": [
    "We'll continue with twitter embeddings. Feel free to sub in a different kind and see what kind of results you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4107874c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============------------------------------------] 28.0% 55.9/199.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====================-----------------------------] 43.6% 87.0/199.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===============================-------------------] 62.7% 125.0/199.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===========================================-------] 86.8% 173.1/199.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================-] 99.7% 198.8/199.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download the model and return as object ready for use\n",
    "embeddings_glove_twitter = api.load(\"glove-twitter-50\")\n",
    "#embeddings = glove_twitter = api.load(\"glove-twitter-100\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab47b07",
   "metadata": {},
   "source": [
    "We can access the embedding of a single word in the dictionary like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0d7bb142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.072944 ,  0.31349  , -0.37301  , -0.74591  ,  0.024118 ,\n",
       "        0.26288  , -0.52766  ,  0.45845  ,  0.66482  , -0.32284  ,\n",
       "        0.070524 , -0.23753  , -1.9064   , -0.12384  ,  0.34087  ,\n",
       "        0.10557  ,  0.64763  , -1.5884   ,  0.28275  , -0.48506  ,\n",
       "        0.37902  , -0.28709  , -0.0066354,  0.24017  , -0.034383 ,\n",
       "       -0.3468   , -0.0061342, -0.12497  , -0.011999 , -0.63745  ,\n",
       "       -0.35676  , -0.17062  , -0.86248  ,  0.18034  ,  0.1995   ,\n",
       "       -0.25941  ,  0.2586   ,  0.17861  ,  0.7617   ,  0.8704   ,\n",
       "       -0.53839  ,  0.38899  ,  0.27174  ,  0.33564  , -0.29995  ,\n",
       "        0.9688   ,  0.15263  , -0.48423  ,  0.70449  ,  0.1936   ,\n",
       "       -0.30026  , -0.46582  , -0.11025  , -1.3443   ,  0.7221   ,\n",
       "        0.41727  ,  0.078679 , -0.34484  , -0.24705  , -0.65691  ,\n",
       "       -0.22723  , -0.68642  ,  0.30941  ,  0.38086  ,  0.029261 ,\n",
       "       -0.22846  , -0.33021  , -0.48214  , -0.28144  , -0.17876  ,\n",
       "        0.071934 ,  0.17553  ,  0.49776  , -0.44887  ,  0.020932 ,\n",
       "        0.15791  ,  0.20464  ,  0.27814  ,  0.3431   ,  0.33648  ,\n",
       "        0.29166  ,  0.33483  , -1.0122   , -0.20511  , -0.51084  ,\n",
       "       -0.11805  , -0.37913  , -0.57463  ,  0.70959  ,  0.15359  ,\n",
       "       -0.32583  ,  0.20893  , -0.79571  , -0.46678  ,  0.43836  ,\n",
       "        0.54177  , -0.18551  ,  1.282    , -0.18846  , -0.098897 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['sup']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86c25ae",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2158ba",
   "metadata": {},
   "source": [
    "## Transform model inputs into vector representation  (x) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b8eb2e",
   "metadata": {},
   "source": [
    "First we need to transform the input text into a vector representation. \n",
    "\n",
    "Let's write a function that constructs an input vector from the training example. It will take a single example in the form of a dictionary OR a comment string, and should have a switch depending on the type of the input. This will help us down the road "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364f8a70",
   "metadata": {},
   "source": [
    "what needs to happen in this function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d60fe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_input(example, embeddings):\n",
    "    \"\"\"\n",
    "    :example: training example from huggingface dataset in the form of a dictionary, e.g.\n",
    "    \n",
    "    {'comment_text': '\"\\nAnd ... I really don\\'t think you understand.  I came here and my idea was bad right away.  What kind of community goes \"\"you have bad ideas\"\" go away, instead of helping rewrite them.   \"',\n",
    "     'toxic': 0,\n",
    "     'severe_toxic': 0,\n",
    "     'obscene': 0,\n",
    "     'threat': 0,\n",
    "     'insult': 0,\n",
    "     'identity_hate': 0}\n",
    "\n",
    "    returns a dictionary with a new key, x, which maps to a vector representing the comment\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(example, str):\n",
    "        raise Exception(\"not implemented\")\n",
    "    else:\n",
    "        raise Exception(\"not implemented\")\n",
    "        \n",
    "    example['x'] = None\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7584dd29",
   "metadata": {},
   "source": [
    "We need to process the input string into words. It makes sense to use the same tokenization function we used for the w2v training corpus, because those are the vectors we have to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e778603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(string):\n",
    "    \"\"\"\n",
    "    tokenization function from n-gram models\n",
    "    \"\"\"\n",
    "    tokenized = re.sub(r'(\\w)([.,?!;:])', r'\\1 \\2', string) \n",
    "    tokenized = tokenized.split()\n",
    "    tokenized = [word.lower() for word in tokenized]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1992c9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_input(example, word_embeddings):\n",
    "    \"\"\"\n",
    "    :example: training example from huggingface dataset in the form of a dictionary, e.g.\n",
    "    \n",
    "    {'comment_text': '\"\\nAnd ... I really don\\'t think you understand.  I came here and my idea was bad right away.  What kind of community goes \"\"you have bad ideas\"\" go away, instead of helping rewrite them.   \"',\n",
    "     'toxic': 0,\n",
    "     'severe_toxic': 0,\n",
    "     'obscene': 0,\n",
    "     'threat': 0,\n",
    "     'insult': 0,\n",
    "     'identity_hate': 0}\n",
    "\n",
    "    returns a dictionary with a new key, x, which maps to a vector representing the comment\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the tokens\n",
    "    if isinstance(example, str):\n",
    "        tokens = tokenize(example)\n",
    "        example = {} # dummy input dict\n",
    "    else:\n",
    "        tokens = tokenize(example['comment_text'])\n",
    "\n",
    "    # get the vectors for each token and average them into one vector\n",
    "    vecs = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec = word_embeddings[word]\n",
    "        except KeyError: # this token is not in our embeddings dictionary\n",
    "            vec = np.zeros(word_embeddings.vector_size)\n",
    "        vecs.append(vec)\n",
    "    \n",
    "    centroid = np.mean(vecs, axis=0)\n",
    "    \n",
    "    # convert numpy array to torch tensor\n",
    "    torch_tensor = torch.from_numpy(centroid).float()\n",
    "    \n",
    "    # return the same example with a new key 'x' containing the vectorized input\n",
    "    example['x'] = torch_tensor\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cb5739",
   "metadata": {},
   "source": [
    "We test out our form_input function and see that it adds a new key to the example dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc87f531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comment_text': \"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\",\n",
       " 'toxic': 0,\n",
       " 'severe_toxic': 0,\n",
       " 'obscene': 0,\n",
       " 'threat': 0,\n",
       " 'insult': 0,\n",
       " 'identity_hate': 0,\n",
       " 'x': tensor([ 1.1892e-01,  1.0474e-01, -7.7170e-02,  2.2719e-02,  2.9379e-02,\n",
       "         -2.0261e-02,  3.7900e-02, -5.0447e-03,  4.9346e-02, -5.7893e-02,\n",
       "          6.4910e-02, -6.5754e-02, -3.2642e+00, -7.3882e-02,  7.8788e-02,\n",
       "         -4.9505e-02,  1.5649e-02, -7.1719e-02, -1.2999e-01, -8.2543e-02,\n",
       "         -1.6375e-01,  5.9585e-02, -4.1713e-03, -3.5399e-02, -8.7974e-02,\n",
       "         -2.8266e-01,  5.8411e-02, -2.8793e-02,  2.0931e-01, -1.2959e-01,\n",
       "         -1.7345e-02,  1.4128e-01, -2.1452e-01,  1.1127e-02,  1.3108e-01,\n",
       "         -9.3802e-02,  2.5561e-02, -9.2482e-03,  1.0009e-01, -2.3536e-03,\n",
       "         -3.7532e-01, -1.7112e-02,  1.1635e-02,  9.7838e-02,  1.3092e-01,\n",
       "         -1.3756e-01,  4.4703e-02, -4.1152e-02, -5.4775e-02, -1.1600e-01,\n",
       "         -3.8759e-02,  4.5651e-02,  2.5019e-02,  2.3615e-02, -8.9351e-02,\n",
       "          7.1356e-02, -2.0147e-01, -3.3502e-02,  5.1916e-02,  1.4815e-01,\n",
       "          8.4042e-02,  3.4844e-02, -6.0027e-02, -4.3814e-02, -4.6929e-02,\n",
       "          3.8623e-02, -1.1159e-01,  4.0602e-02,  3.3896e-02,  3.0237e-02,\n",
       "          8.5768e-02,  1.0566e-01,  7.4333e-02, -6.3125e-02,  6.4795e-02,\n",
       "         -3.7267e-02, -1.8338e-01, -3.8432e-02, -5.8692e-02, -4.3412e-04,\n",
       "          1.1894e+00,  1.3269e-01,  6.5700e-02,  5.6320e-02, -2.1795e-02,\n",
       "         -6.1437e-02, -5.6639e-02,  6.0199e-02,  1.0361e-01, -5.9038e-02,\n",
       "         -1.4365e-01,  1.2784e-01,  7.3334e-02, -1.5017e-01, -7.6098e-02,\n",
       "         -1.5649e-02, -2.4281e-02, -1.7723e-01,  1.6580e-01,  4.7310e-03])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = train[0]\n",
    "form_input(sample, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9513b75",
   "metadata": {},
   "source": [
    "we can also use a regular string as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c3f7df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([ 0.0953,  0.0600,  0.1371,  0.4924, -0.2468, -0.0166,  0.3260, -0.0351,\n",
       "          0.0622,  0.4740,  0.1279, -0.0801, -5.2483, -0.2469, -0.1907, -0.0444,\n",
       "         -0.0053, -0.1239, -0.4338, -0.1554, -0.2208,  0.0270, -0.1308,  0.1786,\n",
       "          0.6365, -0.7580,  0.0469,  0.2287, -0.0327, -0.3872, -0.4441, -0.2317,\n",
       "         -0.1610, -0.0405, -0.4412,  0.3505, -0.1863,  0.3733,  0.4390, -0.3021,\n",
       "         -0.8991,  0.2143, -0.0986, -0.1067,  0.3615, -0.1637,  0.1845,  0.2359,\n",
       "          0.2526,  0.2617, -0.2349,  0.2826,  0.2627, -0.3209,  0.6101,  0.0562,\n",
       "         -0.6920,  0.0256, -0.1111,  0.4397,  0.1249,  0.0639, -0.0421,  0.2151,\n",
       "          0.0659, -0.1925, -0.5575,  0.4118,  0.0263,  0.2235,  0.1402,  0.3073,\n",
       "          0.1506, -0.0555, -0.2228,  0.0847,  0.4378, -0.2328, -0.4477,  0.2623,\n",
       "          1.8900,  0.1061, -0.5123, -0.1454,  0.2336,  0.1836, -0.5236, -0.0297,\n",
       "         -0.2249, -0.1376, -0.2505,  0.0524, -0.1484,  0.4463, -0.1469,  0.5200,\n",
       "         -0.2752, -0.1547,  0.6082, -0.0178])}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "form_input(\"fuck you you fucking fuck\", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f865d669",
   "metadata": {},
   "source": [
    "## Preprocessing: Model Outputs\n",
    "\n",
    "We need to transform the output label into vector representation (gold y). \n",
    "\n",
    "### Question: What is our golf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "188fd769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_output(example):\n",
    "    example['y'] = example['toxic']\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "757e6d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comment_text': \"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\",\n",
       " 'toxic': 0,\n",
       " 'severe_toxic': 0,\n",
       " 'obscene': 0,\n",
       " 'threat': 0,\n",
       " 'insult': 0,\n",
       " 'identity_hate': 0,\n",
       " 'x': tensor([ 1.1892e-01,  1.0474e-01, -7.7170e-02,  2.2719e-02,  2.9379e-02,\n",
       "         -2.0261e-02,  3.7900e-02, -5.0447e-03,  4.9346e-02, -5.7893e-02,\n",
       "          6.4910e-02, -6.5754e-02, -3.2642e+00, -7.3882e-02,  7.8788e-02,\n",
       "         -4.9505e-02,  1.5649e-02, -7.1719e-02, -1.2999e-01, -8.2543e-02,\n",
       "         -1.6375e-01,  5.9585e-02, -4.1713e-03, -3.5399e-02, -8.7974e-02,\n",
       "         -2.8266e-01,  5.8411e-02, -2.8793e-02,  2.0931e-01, -1.2959e-01,\n",
       "         -1.7345e-02,  1.4128e-01, -2.1452e-01,  1.1127e-02,  1.3108e-01,\n",
       "         -9.3802e-02,  2.5561e-02, -9.2482e-03,  1.0009e-01, -2.3536e-03,\n",
       "         -3.7532e-01, -1.7112e-02,  1.1635e-02,  9.7838e-02,  1.3092e-01,\n",
       "         -1.3756e-01,  4.4703e-02, -4.1152e-02, -5.4775e-02, -1.1600e-01,\n",
       "         -3.8759e-02,  4.5651e-02,  2.5019e-02,  2.3615e-02, -8.9351e-02,\n",
       "          7.1356e-02, -2.0147e-01, -3.3502e-02,  5.1916e-02,  1.4815e-01,\n",
       "          8.4042e-02,  3.4844e-02, -6.0027e-02, -4.3814e-02, -4.6929e-02,\n",
       "          3.8623e-02, -1.1159e-01,  4.0602e-02,  3.3896e-02,  3.0237e-02,\n",
       "          8.5768e-02,  1.0566e-01,  7.4333e-02, -6.3125e-02,  6.4795e-02,\n",
       "         -3.7267e-02, -1.8338e-01, -3.8432e-02, -5.8692e-02, -4.3412e-04,\n",
       "          1.1894e+00,  1.3269e-01,  6.5700e-02,  5.6320e-02, -2.1795e-02,\n",
       "         -6.1437e-02, -5.6639e-02,  6.0199e-02,  1.0361e-01, -5.9038e-02,\n",
       "         -1.4365e-01,  1.2784e-01,  7.3334e-02, -1.5017e-01, -7.6098e-02,\n",
       "         -1.5649e-02, -2.4281e-02, -1.7723e-01,  1.6580e-01,  4.7310e-03]),\n",
       " 'y': 0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "form_output(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53abe894",
   "metadata": {},
   "source": [
    "# Building the Pytorch Model\n",
    "\n",
    "We have several decisions to make.\n",
    "\n",
    "- What is the shape of the input?\n",
    "- What is the shape of the output?\n",
    "- What form do we want the output to take? \n",
    "- How many layers do we want the model to have?\n",
    "- What activation function should we use at each layer?\n",
    "- Do we use dropout?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb42c308",
   "metadata": {},
   "source": [
    "We subclass nn.Module (which itself is a class and able to keep track of state). In this case, we want to create a class that holds our weights, bias, and method for the forward step. nn.Module has a number of attributes and methods (such as .parameters() and .zero_grad()) which we will be using.\n",
    "\n",
    "We also can store the loss function here as well.\n",
    "\n",
    "Here is an example FFNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9e29833",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines the core neural network for doing binary classification over a single datapoint at a time. This consists\n",
    "    of matrix multiplication, tanh nonlinearity, another matrix multiplication, and then\n",
    "    a sigmoid layer to give the ouputs.\n",
    "\n",
    "    The forward() function does the important computation. The backward() method is inherited from nn.Module and\n",
    "    handles backpropagation.\n",
    "    \"\"\"\n",
    "    def __init__(self, word_embeddings, inp, hid, out):\n",
    "        \"\"\"\n",
    "        Constructs the computation graph by instantiating the various layers and initializing weights.\n",
    "\n",
    "        :param inp: size of input (integer)\n",
    "        :param hid: size of hidden layer(integer)\n",
    "        :param out: size of output (integer), which should be the number of classes\n",
    "        \"\"\"\n",
    "        super(FFNN, self).__init__()\n",
    "        self.V = nn.Linear(inp, hid)\n",
    "        self.g = nn.Tanh()\n",
    "        #self.g = nn.ReLU()\n",
    "        self.W = nn.Linear(hid, out)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Initialize weights according to a formula due to Xavier Glorot.\n",
    "        nn.init.xavier_uniform_(self.V.weight)\n",
    "        nn.init.xavier_uniform_(self.W.weight)\n",
    "        \n",
    "        self.num_classes = out\n",
    "        self.loss = nn.BCELoss()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Runs the neural network on the given data and returns log probabilities of the various classes.\n",
    "\n",
    "        :param x: a [inp]-sized tensor of input data\n",
    "        :return: an [out]-sized tensor of log probabilities. (In general your network can be set up to return either log\n",
    "        probabilities or a tuple of (loss, log probability) if you want to pass in y to this function as well\n",
    "        \"\"\"\n",
    "        return self.sigmoid(self.W(self.g(self.V(x))))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac6cee0",
   "metadata": {},
   "source": [
    "Since we’re now using an object instead of just using a function, we first have to instantiate our model\n",
    "\n",
    "- how many input dimensions does it have? output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a50ef9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNN(\n",
      "  (V): Linear(in_features=100, out_features=300, bias=True)\n",
      "  (g): Tanh()\n",
      "  (W): Linear(in_features=300, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (loss): BCELoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = FFNN(embeddings, 100,300,1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15769922",
   "metadata": {},
   "source": [
    "let's run the forward function to compute the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae77c706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'comment_text': \"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\", 'toxic': 0, 'severe_toxic': 0, 'obscene': 0, 'threat': 0, 'insult': 0, 'identity_hate': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.4174], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = train[0]\n",
    "print(example)\n",
    "inp = form_input(example, embeddings)\n",
    "outp = model.forward(inp['x'])\n",
    "outp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88305dc",
   "metadata": {},
   "source": [
    "we can access the logits of the returned value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d99d31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4174])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed30ae11",
   "metadata": {},
   "source": [
    "And we can change the PyTorch tensor to a Numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00b59a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.41742533], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40dc1bf",
   "metadata": {},
   "source": [
    "# Making Predictions\n",
    "\n",
    "The final layer is a continuous, differentiable function. It will predict a number between 0 and 1. But we want our output to be either 0 or 1 with no in between. How do we transform this output logit into a prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d2aca0",
   "metadata": {},
   "source": [
    "Write a function called predict() that will generate a prediction that is either a 0 or a 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c8ea42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, word_embeddings, example) -> int:\n",
    "    x = form_input(example, word_embeddings)['x']\n",
    "    log_probs = model.forward(x)\n",
    "    \n",
    "    if log_probs < 0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed8e2ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = predict(model, embeddings, \"fuck you you fucking fuck\")\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578dd9d6",
   "metadata": {},
   "source": [
    "The model doesn't know what google means by toxicity. we have to train it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b1fad5",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9400af01",
   "metadata": {},
   "source": [
    "First, a way to evaluate the model performance on a whole bunch of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d146bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation(golds, predictions):\n",
    "    \"\"\"\n",
    "    Prints evaluation statistics comparing golds and predictions, each of which is a sequence of 0/1 labels.\n",
    "    Prints accuracy as well as precision/recall/F1 of the positive class, which can sometimes be informative if either\n",
    "    the golds or predictions are highly biased.\n",
    "\n",
    "    :param golds: gold labels, list of ints\n",
    "    :param predictions: pred labels, list of ints\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    num_correct = 0\n",
    "    num_pos_correct = 0\n",
    "    num_pred = 0\n",
    "    num_gold = 0\n",
    "    num_total = 0\n",
    "    if len(golds) != len(predictions):\n",
    "        raise Exception(\"Mismatched gold/pred lengths: %i / %i\" % (len(golds), len(predictions)))\n",
    "    for idx in range(0, len(golds)):\n",
    "        gold = golds[idx]\n",
    "        #print(\"gold \", gold)\n",
    "        prediction = predictions[idx]\n",
    "        #print(\"prediction \", prediction)\n",
    "        if prediction == gold:\n",
    "            num_correct += 1\n",
    "        if prediction == 1:\n",
    "            num_pred += 1\n",
    "        if gold == 1:\n",
    "            num_gold += 1\n",
    "        if prediction == 1 and gold == 1:\n",
    "            num_pos_correct += 1\n",
    "        num_total += 1\n",
    "    acc = float(num_correct) / num_total\n",
    "    output_str = \"Accuracy: %i / %i = %f\" % (num_correct, num_total, acc)\n",
    "    prec = float(num_pos_correct) / num_pred if num_pred > 0 else 0.0\n",
    "    rec = float(num_pos_correct) / num_gold if num_gold > 0 else 0.0\n",
    "    f1 = 2 * prec * rec / (prec + rec) if prec > 0 and rec > 0 else 0.0\n",
    "    output_str += \";\\nPrecision (fraction of predicted positives that are correct): %i / %i = %f\" % (num_pos_correct, num_pred, prec)\n",
    "    output_str += \";\\nRecall (fraction of true positives predicted correctly): %i / %i = %f\" % (num_pos_correct, num_gold, rec)\n",
    "    output_str += \";\\nF1 (harmonic mean of precision and recall): %f;\\n\" % f1\n",
    "    print(output_str)\n",
    "    return acc, f1, output_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42818267",
   "metadata": {},
   "source": [
    "# Lab: Coding the Training Loop\n",
    "\n",
    "Your task will be to write the training loop for training the feed forward neural network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f645fd7",
   "metadata": {},
   "source": [
    "### Get the data into the right shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb7f653",
   "metadata": {},
   "source": [
    "huggingface datasets has a .map() function that will alter our dataset in place. it asks for a function as its input.\n",
    "that function has to be of a particular type. in particular, it has to expect something of the same type as a single example (a dictionary) and return a dictionary. We define an anonymous function using a lambda expression that uses our form_input function. no need to worry about the details here unless you are interested because type calculus is sick and so are lambda expressions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ab866c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                       \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'comment_text': \"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\",\n",
       " 'toxic': 0,\n",
       " 'severe_toxic': 0,\n",
       " 'obscene': 0,\n",
       " 'threat': 0,\n",
       " 'insult': 0,\n",
       " 'identity_hate': 0,\n",
       " 'x': [0.1189161166548729,\n",
       "  0.1047351211309433,\n",
       "  -0.07717016339302063,\n",
       "  0.022719241678714752,\n",
       "  0.02937931939959526,\n",
       "  -0.020260874181985855,\n",
       "  0.03789982572197914,\n",
       "  -0.005044739227741957,\n",
       "  0.049345921725034714,\n",
       "  -0.05789319425821304,\n",
       "  0.06490956246852875,\n",
       "  -0.06575418263673782,\n",
       "  -3.2642297744750977,\n",
       "  -0.07388174533843994,\n",
       "  0.07878798246383667,\n",
       "  -0.04950515925884247,\n",
       "  0.015649378299713135,\n",
       "  -0.07171885669231415,\n",
       "  -0.12999308109283447,\n",
       "  -0.08254311978816986,\n",
       "  -0.1637531965970993,\n",
       "  0.05958490073680878,\n",
       "  -0.004171306267380714,\n",
       "  -0.035398997366428375,\n",
       "  -0.08797356486320496,\n",
       "  -0.2826557457447052,\n",
       "  0.05841091647744179,\n",
       "  -0.028793126344680786,\n",
       "  0.20930999517440796,\n",
       "  -0.12959198653697968,\n",
       "  -0.01734531857073307,\n",
       "  0.1412840038537979,\n",
       "  -0.21452048420906067,\n",
       "  0.011127372272312641,\n",
       "  0.1310827136039734,\n",
       "  -0.09380161762237549,\n",
       "  0.025560639798641205,\n",
       "  -0.009248169139027596,\n",
       "  0.10009437799453735,\n",
       "  -0.0023536207154393196,\n",
       "  -0.37531548738479614,\n",
       "  -0.017111986875534058,\n",
       "  0.011635467410087585,\n",
       "  0.09783836454153061,\n",
       "  0.13092026114463806,\n",
       "  -0.13756363093852997,\n",
       "  0.04470345377922058,\n",
       "  -0.04115219786763191,\n",
       "  -0.05477544665336609,\n",
       "  -0.11600277572870255,\n",
       "  -0.038758549839258194,\n",
       "  0.04565086588263512,\n",
       "  0.025019152089953423,\n",
       "  0.023614641278982162,\n",
       "  -0.08935070037841797,\n",
       "  0.07135596126317978,\n",
       "  -0.20147335529327393,\n",
       "  -0.03350183740258217,\n",
       "  0.051915593445301056,\n",
       "  0.14815489947795868,\n",
       "  0.08404234051704407,\n",
       "  0.03484421968460083,\n",
       "  -0.060026537626981735,\n",
       "  -0.043814338743686676,\n",
       "  -0.04692871496081352,\n",
       "  0.03862345963716507,\n",
       "  -0.111594058573246,\n",
       "  0.04060158133506775,\n",
       "  0.03389601781964302,\n",
       "  0.030237041413784027,\n",
       "  0.08576776087284088,\n",
       "  0.10566014051437378,\n",
       "  0.07433313131332397,\n",
       "  -0.0631251409649849,\n",
       "  0.06479521840810776,\n",
       "  -0.03726732358336449,\n",
       "  -0.18338310718536377,\n",
       "  -0.038431841880083084,\n",
       "  -0.058691930025815964,\n",
       "  -0.0004341150925029069,\n",
       "  1.1893630027770996,\n",
       "  0.13269193470478058,\n",
       "  0.06570015847682953,\n",
       "  0.05632006376981735,\n",
       "  -0.021794898435473442,\n",
       "  -0.061437416821718216,\n",
       "  -0.05663856863975525,\n",
       "  0.060199201107025146,\n",
       "  0.10360666364431381,\n",
       "  -0.059037841856479645,\n",
       "  -0.14365164935588837,\n",
       "  0.12784436345100403,\n",
       "  0.07333448529243469,\n",
       "  -0.15017351508140564,\n",
       "  -0.07609803974628448,\n",
       "  -0.015649329870939255,\n",
       "  -0.02428101748228073,\n",
       "  -0.17723391950130463,\n",
       "  0.16579629480838776,\n",
       "  0.004731019027531147]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda ex: form_input(ex, embeddings))\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0512b34e",
   "metadata": {},
   "source": [
    "now our dataset has an extra column with vector representations of each example. we do this once ahead of time rather than on the fly because we will iterate through our training and dev sets many times, and we don't want waste time running form_input over and over"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02c085b",
   "metadata": {},
   "source": [
    "We do the same thing with form_output. clearly, we didnt need separate functions, but it helps to separate our thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "937a6998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                       \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'comment_text': \"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\",\n",
       " 'toxic': 0,\n",
       " 'severe_toxic': 0,\n",
       " 'obscene': 0,\n",
       " 'threat': 0,\n",
       " 'insult': 0,\n",
       " 'identity_hate': 0,\n",
       " 'x': [0.1189161166548729,\n",
       "  0.1047351211309433,\n",
       "  -0.07717016339302063,\n",
       "  0.022719241678714752,\n",
       "  0.02937931939959526,\n",
       "  -0.020260874181985855,\n",
       "  0.03789982572197914,\n",
       "  -0.005044739227741957,\n",
       "  0.049345921725034714,\n",
       "  -0.05789319425821304,\n",
       "  0.06490956246852875,\n",
       "  -0.06575418263673782,\n",
       "  -3.2642297744750977,\n",
       "  -0.07388174533843994,\n",
       "  0.07878798246383667,\n",
       "  -0.04950515925884247,\n",
       "  0.015649378299713135,\n",
       "  -0.07171885669231415,\n",
       "  -0.12999308109283447,\n",
       "  -0.08254311978816986,\n",
       "  -0.1637531965970993,\n",
       "  0.05958490073680878,\n",
       "  -0.004171306267380714,\n",
       "  -0.035398997366428375,\n",
       "  -0.08797356486320496,\n",
       "  -0.2826557457447052,\n",
       "  0.05841091647744179,\n",
       "  -0.028793126344680786,\n",
       "  0.20930999517440796,\n",
       "  -0.12959198653697968,\n",
       "  -0.01734531857073307,\n",
       "  0.1412840038537979,\n",
       "  -0.21452048420906067,\n",
       "  0.011127372272312641,\n",
       "  0.1310827136039734,\n",
       "  -0.09380161762237549,\n",
       "  0.025560639798641205,\n",
       "  -0.009248169139027596,\n",
       "  0.10009437799453735,\n",
       "  -0.0023536207154393196,\n",
       "  -0.37531548738479614,\n",
       "  -0.017111986875534058,\n",
       "  0.011635467410087585,\n",
       "  0.09783836454153061,\n",
       "  0.13092026114463806,\n",
       "  -0.13756363093852997,\n",
       "  0.04470345377922058,\n",
       "  -0.04115219786763191,\n",
       "  -0.05477544665336609,\n",
       "  -0.11600277572870255,\n",
       "  -0.038758549839258194,\n",
       "  0.04565086588263512,\n",
       "  0.025019152089953423,\n",
       "  0.023614641278982162,\n",
       "  -0.08935070037841797,\n",
       "  0.07135596126317978,\n",
       "  -0.20147335529327393,\n",
       "  -0.03350183740258217,\n",
       "  0.051915593445301056,\n",
       "  0.14815489947795868,\n",
       "  0.08404234051704407,\n",
       "  0.03484421968460083,\n",
       "  -0.060026537626981735,\n",
       "  -0.043814338743686676,\n",
       "  -0.04692871496081352,\n",
       "  0.03862345963716507,\n",
       "  -0.111594058573246,\n",
       "  0.04060158133506775,\n",
       "  0.03389601781964302,\n",
       "  0.030237041413784027,\n",
       "  0.08576776087284088,\n",
       "  0.10566014051437378,\n",
       "  0.07433313131332397,\n",
       "  -0.0631251409649849,\n",
       "  0.06479521840810776,\n",
       "  -0.03726732358336449,\n",
       "  -0.18338310718536377,\n",
       "  -0.038431841880083084,\n",
       "  -0.058691930025815964,\n",
       "  -0.0004341150925029069,\n",
       "  1.1893630027770996,\n",
       "  0.13269193470478058,\n",
       "  0.06570015847682953,\n",
       "  0.05632006376981735,\n",
       "  -0.021794898435473442,\n",
       "  -0.061437416821718216,\n",
       "  -0.05663856863975525,\n",
       "  0.060199201107025146,\n",
       "  0.10360666364431381,\n",
       "  -0.059037841856479645,\n",
       "  -0.14365164935588837,\n",
       "  0.12784436345100403,\n",
       "  0.07333448529243469,\n",
       "  -0.15017351508140564,\n",
       "  -0.07609803974628448,\n",
       "  -0.015649329870939255,\n",
       "  -0.02428101748228073,\n",
       "  -0.17723391950130463,\n",
       "  0.16579629480838776,\n",
       "  0.004731019027531147],\n",
       " 'y': 0}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda ex: form_output(ex))\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cf65fd",
   "metadata": {},
   "source": [
    "### Dataset Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eef4e97",
   "metadata": {},
   "source": [
    "Remember, the data are already split into train and test sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33883612",
   "metadata": {},
   "source": [
    "It's normal to have a third small data split called val (validate) or dev, that we use to evaluate the model while the training loop is running. Then, at the end of training, we evaluate on the test set. The dev set can be used to do things like choose **hyperparameters** for the model---we can't use the test set for this because then we'd be 'cheating'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c0bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "\n",
    "train = dataset['train']\n",
    "dev = dataset['test']\n",
    "test = dataset['test']\n",
    "\n",
    "print(repr(len(train)) + \" / \" + repr(len(dev)) + \" / \" + repr(len(test)) + \" train/dev/test examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd0b46",
   "metadata": {},
   "source": [
    "Let's set the model hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cddb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "\n",
    "num_epochs = 10\n",
    "hidden_size = 200\n",
    "initial_learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7360659",
   "metadata": {},
   "source": [
    "We initialize a model and an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340faf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffnn = FFNN(embeddings, embeddings.vector_size, hidden_size, 1)\n",
    "optimizer = optim.Adam(ffnn.parameters(), lr=initial_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8473b6",
   "metadata": {},
   "source": [
    "Next we need to code the actual training loop. This loop will run through the training data for a set number of epochs. In each epoch, it should iterate through all of the training examples. Ideally, it will do this in a random order every time. You should run the example through the model, calculate the loss, and then use the loss to run the backward step. **be sure to zero out the gradients before calling the forward step!!** This is done with `ffnn.zero_grad()` Our loss function is stored in the FFNN object, and can be called using `ffnn.loss()`. The backpropagation step is run on the loss, i.e. `loss.backward()` (where `loss` is the loss computed at this time step). At the end of each epoch, the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f1ff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## training loop for toxicity classification\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    ex_indices = [i for i in range(0, len(train))]\n",
    "    random.shuffle(ex_indices)\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    ffnn.train()\n",
    "    for idx in ex_indices:\n",
    "        \n",
    "        x = torch.Tensor(train[idx]['x'])\n",
    "        \n",
    "        #print(x)\n",
    "        y = train[idx]['y']\n",
    "        # transform to torch\n",
    "        y = torch.from_numpy(np.asarray(y,dtype=np.float32)).unsqueeze(-1)\n",
    "        \n",
    "        # Build one-hot representation of y. Instead of the label 0 or 1, y_onehot is either [0, 1] or [1, 0]. This\n",
    "        # way we can take the dot product directly with a probability vector to get class probabilities.\n",
    "        #y_onehot = torch.zeros(ffnn.num_classes)\n",
    "        \n",
    "        # scatter will write the value of 1 into the position of y_onehot given by y\n",
    "        #y_onehot.scatter_(0, torch.from_numpy(np.asarray(y,dtype=np.int64)), 1)\n",
    "        \n",
    "        # Zero out the gradients from the FFNN object. *THIS IS VERY IMPORTANT TO DO BEFORE CALLING BACKWARD()*\n",
    "        ffnn.zero_grad()\n",
    "        y_hat = ffnn.forward(x)\n",
    "        \n",
    "        # Can also use built-in NLLLoss as a shortcut here but we're being explicit here\n",
    "        loss = ffnn.loss(y_hat, y)\n",
    "        total_loss += loss\n",
    "        \n",
    "        # Computes the gradient and takes the optimizer step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Total loss on epoch %s: %f\" % (epoch, total_loss))\n",
    "    \n",
    "    ffnn.eval()\n",
    "    dev_y_hats = [predict(ffnn, embeddings, ex['comment_text']) for ex in dev]\n",
    "    print_evaluation(dev['y'], dev_y_hats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9403bf68",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e74115e",
   "metadata": {},
   "source": [
    "Let's use the model to make some predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eca2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(ffnn, embeddings, \"fuck you you fucking fuck\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11baa63f",
   "metadata": {},
   "source": [
    "# Improving the Model with Features\n",
    "\n",
    "This works okay. How can we get better performance? See if you can improve the model to X F1 score. You could try using different embeddings. You could change the model itself, adding more layers or trying a different activation function. You could add dropout after one of the layers (this takes one line to implement with pytorch, but you might have to look up about what it is or how it's used). You can also add hand-crafted features in addition to the embeddings to serve as input. This is a very common strategy and usually helps a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2983a42a",
   "metadata": {},
   "source": [
    "Instead of just representing the input as word embeddings, we can represent the input as a vector of word embeddings concatenated with additional dimensions that encode other variables we are interested in. The features can be real-valued, like: how many tokens are in the example? what's the average word length? Features can also be binary, like: does the sentence contain a question mark? \n",
    "\n",
    "For example, s length. Here is a new form_input that adds an average word length feature and a contains X feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f330ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tokens):\n",
    "    question = 1 if True else 0\n",
    "    features = {\n",
    "        \"avg_word_length\": np.mean([len(t) for t in tokens]),\n",
    "        \"contains_q\": 1 if \"?\" in tokens else 0\n",
    "    }\n",
    "    return features\n",
    "\n",
    "def form_input_updated(example, word_embeddings):\n",
    "    \"\"\"\n",
    "    :example: training example from huggingface dataset in the form of a dictionary, e.g.\n",
    "    \n",
    "    {'comment_text': '\"\\nAnd ... I really don\\'t think you understand.  I came here and my idea was bad right away.  What kind of community goes \"\"you have bad ideas\"\" go away, instead of helping rewrite them.   \"',\n",
    "     'toxic': 0,\n",
    "     'severe_toxic': 0,\n",
    "     'obscene': 0,\n",
    "     'threat': 0,\n",
    "     'insult': 0,\n",
    "     'identity_hate': 0}\n",
    "\n",
    "    returns a dictionary with a new key, x, which maps to a vector representing the comment\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(example, str):\n",
    "        tokens = tokenize(example)\n",
    "        example = {} # dummy input dict\n",
    "    else:\n",
    "        tokens = tokenize(example['comment_text'])\n",
    "\n",
    "    vecs = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec = word_embeddings[word]\n",
    "        except KeyError: # this token is not in our embeddings dictionary\n",
    "            vec = np.zeros(word_embeddings.vector_size)\n",
    "        vecs.append(vec)\n",
    "    \n",
    "    centroid = np.mean(vecs, axis=0)\n",
    "    \n",
    "    \n",
    "    features_dict = extract_features(tokens)\n",
    "    features_list = list(features_dict.values())\n",
    "    \n",
    "    final_vector = np.append(centroid, features_list, axis=0)\n",
    "    \n",
    "    # we need torch form which is a tensor, not a numpy array\n",
    "    \n",
    "    torch_tensor = torch.from_numpy(final_vector).float()\n",
    "    example['x'] = torch_tensor\n",
    "    return example\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a024e655",
   "metadata": {},
   "outputs": [],
   "source": [
    "form_input_updated(\"hello my name is fhwfhjwhfjwklfjwj4klwfkl?\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6102e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-form our inputs, make a new model and train that one. \n",
    "\n",
    "ffnn = FFNN(embeddings, embeddings.vector_size, hidden_size, 1)\n",
    "optimizer = optim.Adam(ffnn.parameters(), lr=initial_learning_rate)\n",
    "\n",
    "dataset = dataset.map(lambda ex: form_input_updated(ex, embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1487e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0][\"x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a3a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## training loop for toxicity classification\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    ex_indices = [i for i in range(0, len(train))]\n",
    "    random.shuffle(ex_indices)\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    ffnn.train()\n",
    "    for idx in ex_indices:\n",
    "        \n",
    "        x = torch.Tensor(train[idx]['x'])\n",
    "        \n",
    "        #print(x)\n",
    "        y = train[idx]['y']\n",
    "        # transform to torch\n",
    "        y = torch.from_numpy(np.asarray(y,dtype=np.float32)).unsqueeze(-1)\n",
    "        \n",
    "        # Build one-hot representation of y. Instead of the label 0 or 1, y_onehot is either [0, 1] or [1, 0]. This\n",
    "        # way we can take the dot product directly with a probability vector to get class probabilities.\n",
    "        #y_onehot = torch.zeros(ffnn.num_classes)\n",
    "        \n",
    "        # scatter will write the value of 1 into the position of y_onehot given by y\n",
    "        #y_onehot.scatter_(0, torch.from_numpy(np.asarray(y,dtype=np.int64)), 1)\n",
    "        \n",
    "        # Zero out the gradients from the FFNN object. *THIS IS VERY IMPORTANT TO DO BEFORE CALLING BACKWARD()*\n",
    "        ffnn.zero_grad()\n",
    "        y_hat = ffnn.forward(x)\n",
    "        \n",
    "        # Can also use built-in NLLLoss as a shortcut here but we're being explicit here\n",
    "        loss = ffnn.loss(y_hat, y)\n",
    "        total_loss += loss\n",
    "        \n",
    "        # Computes the gradient and takes the optimizer step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Total loss on epoch %s: %f\" % (epoch, total_loss))\n",
    "    \n",
    "    ffnn.eval()\n",
    "    dev_y_hats = [predict(ffnn, embeddings, ex['comment_text']) for ex in dev]\n",
    "    print_evaluation(dev['y'], dev_y_hats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90d4418",
   "metadata": {},
   "source": [
    "# Appendix: Example Training Loop\n",
    "\n",
    "Here is an example training loop for learning the XOR function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1d222f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE THE DATA\n",
    "# Synthetic data for XOR: y = x1 XOR x2\n",
    "train_xs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "train_ys = np.array([0, 1, 1, 0], dtype=np.float32)\n",
    "\n",
    "# Define some constants\n",
    "# Inputs are of size 2\n",
    "feat_vec_size = 2\n",
    "\n",
    "# Let's use 4 hidden units\n",
    "embedding_size = 4\n",
    "\n",
    "# We're using 2 classes. What's presented here is multi-class code that can scale to more classes, though\n",
    "# slightly more compact code for the binary case is possible.\n",
    "num_classes = 1\n",
    "\n",
    "\n",
    "# set hyperparameters\n",
    "num_epochs = 100\n",
    "ffnn = FFNN(feat_vec_size, embedding_size, num_classes)\n",
    "initial_learning_rate = 0.1\n",
    "optimizer = optim.Adam(ffnn.parameters(), lr=initial_learning_rate)\n",
    "\n",
    "\n",
    "# RUN TRAINING\n",
    "for epoch in range(0, num_epochs):\n",
    "    \n",
    "    ex_indices = [i for i in range(0, len(train_xs))]\n",
    "    random.shuffle(ex_indices)\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for idx in ex_indices:\n",
    "        x =  torch.from_numpy(train_xs[idx]).float()\n",
    "        y = train_ys[idx]\n",
    "        y = torch.from_numpy(np.asarray(y,dtype=np.float32)).unsqueeze(-1)\n",
    "\n",
    "        # Build one-hot representation of y. Instead of the label 0 or 1, y_onehot is either [0, 1] or [1, 0]. This\n",
    "        # way we can take the dot product directly with a probability vector to get class probabilities.\n",
    "        #y_onehot = torch.zeros(num_classes)\n",
    "        \n",
    "        # scatter will write the value of 1 into the position of y_onehot given by y\n",
    "        #y_onehot.scatter_(0, torch.from_numpy(np.asarray(y,dtype=np.int64)), 1)\n",
    "        # Zero out the gradients from the FFNN object. *THIS IS VERY IMPORTANT TO DO BEFORE CALLING BACKWARD()*\n",
    "        ffnn.zero_grad()\n",
    "        log_probs = ffnn.forward(x)\n",
    "        \n",
    "        print(x)\n",
    "        print(log_probs)\n",
    "        print(y)\n",
    "        \n",
    "        # Can also use built-in NLLLoss as a shortcut here but we're being explicit here\n",
    "        loss = ffnn.loss(log_probs, y)\n",
    "        \n",
    "        total_loss += loss\n",
    "        # Computes the gradient and takes the optimizer step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Total loss on epoch %i: %f\" % (epoch, total_loss))\n",
    "    \n",
    "\n",
    "# Evaluate on the train set\n",
    "# RUN TRAINING AND TEST\n",
    "\n",
    "train_correct = 0\n",
    "for idx in range(0, len(train_xs)):\n",
    "    x = torch.from_numpy(train_xs[idx]).float()\n",
    "    y = train_ys[idx]\n",
    "    log_probs = ffnn.forward(x)\n",
    "    prediction = 1 if log_probs > 0.5 else 0\n",
    "    if y == prediction:\n",
    "        train_correct += 1\n",
    "    print(\"Example \" + repr(train_xs[idx]) + \"; gold = \" + repr(train_ys[idx]) + \"; pred = \" +\\\n",
    "          repr(prediction) + \" with probs \" + repr(log_probs))\n",
    "print(repr(train_correct) + \"/\" + repr(len(train_ys)) + \" correct after training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccacf3f",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- https://vijaygadepalli.medium.com/toxic-comments-classification-696603741872\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219db96a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
